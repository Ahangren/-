一、git_score函数：计算注意力分数
"""
在 torch.einsum('ibhd,jbhd->ijbh', query, key) 中，query 和 key 的形状变化及计算逻辑如下：

输入张量形状假设
假设：

query 的形状为 (i, b, h, d)
i：目标序列长度（如解码端的 token 数）。
b：batch size。
h：注意力头的数量（多头注意力）。
d：每个注意力头的维度（d_k 或 d_q）。
key 的形状为 (j, b, h, d)
j：源序列长度（如编码端的 token 数）。
b, h, d 含义与 query 相同。
爱因斯坦求和规则解析
下标规则 'ibhd,jbhd->ijbh' 的分解：

输入张量的标记：
query 的维度标记为 i, b, h, d。
key 的维度标记为 j, b, h, d。
重复下标 b, h, d：
这些下标在输入中重复出现，表示在这些维度上保持对齐（不求和）。
只有 d 是重复的且未出现在输出中，因此会沿 d 维度求和（点积操作）。
输出形状 ijbh：
输出保留 i, j, b, h 维度，即对每个 batch（b）、每个注意力头（h），计算 query 的第 i 个位置与 key 的第 j 个位置的注意力分数。
计算过程
点积求和：
对 query 和 key 的最后一个维度 d 做点积（求和），得到未归一化的注意力分数。
公式：
output[i,j,b,h]=
d
∑

 query[i,b,h,d]⋅key[j,b,h,d]
输出形状：
结果为 (i, j, b, h)，表示：
对 batch 中每个样本（b），每个注意力头（h），query 的第 i 个位置与 key 的第 j 个位置的相似度分数。

"""


二、prepare_mask函数，掩码处理函数
 假设的输入形状
query_shape = (query_seq_len, batch_size, num_heads, d_k)
key_shape = (key_seq_len, batch_size, num_heads, d_k)
mask 的形状通常为 (batch_size, key_seq_len, query_seq_len) 或其广播形式。
Assert 语句解析
1. assert mask.shape[0] == 1 or mask.shape[0] == query_shape[0]
检查维度 0（mask.shape[0]）：
mask.shape[0] 对应 batch_size（或可广播的维度）。
条件要求：mask 的 batch_size 必须为 1（支持广播到所有样本）或等于 query 的 batch_size（即 query_shape[0]）。
为什么？
如果 mask 的 batch_size=1，PyTorch 会自动广播到所有样本；否则需严格匹配 query 的 batch 维度。
2. assert mask.shape[1] == key_shape[0]
检查维度 1（mask.shape[1]）：
mask.shape[1] 必须等于 key 的序列长度（key_seq_len）。
为什么？
注意力机制中，mask 的该维度用于屏蔽 key 的无效位置（如填充符 PAD），因此必须与 key 的序列长度一致。
3. assert mask.shape[2] == 1 or mask.shape[2] == query_shape[1]
检查维度 2（mask.shape[2]）：
mask.shape[2] 对应 query_seq_len（或可广播的维度）。
条件要求：mask 的该维度必须为 1（支持广播到所有查询位置）或等于 query 的序列长度（query_shape[1]）。
为什么？
如果 mask.shape[2]=1，表示所有查询位置共享同一掩码规则（如解码时的因果掩码）；否则需为每个查询位置单独指定掩码。
典型场景示例
编码器自注意力：
query_seq_len = key_seq_len（如输入序列长度），mask 形状通常为 (batch_size, seq_len, seq_len)。
所有 assert 条件均需严格匹配（无广播）。
解码器的因果掩码：
mask 形状可能为 (1, seq_len, seq_len) 或 (batch_size, seq_len, 1)，利用广播机制屏蔽未来位置。
允许 mask.shape[0]=1 或 mask.shape[2]=1。
跨注意力（如 Encoder-Decoder）：
query_seq_len（解码端）≠ key_seq_len（编码端），mask 形状需为 (batch_size, key_seq_len, query_seq_len)。
为什么需要这些检查？
广播兼容性：确保 mask 能正确广播到 (batch_size, key_seq_len, query_seq_len)。
逻辑正确性：防止因形状不匹配导致无效的注意力计算（如错误屏蔽或遗漏位置）。
性能优化：避免隐式的张量复制（如不合法的广播会触发额外内存分配）。

三、forward函数：训练
"""
这段代码实现了一个 Transformer 风格的多头注意力机制，主要包括 Query-Key-Value 投影、注意力分数计算、掩码处理、Softmax 归一化、注意力权重应用和输出投影。下面逐步解析其逻辑：

1. 输入形状处理
python
seq_len, batch_size, _ = query.shape
输入 query 形状：(seq_len, batch_size, embed_dim)
seq_len：序列长度（如 token 数量）。
batch_size：批大小。
embed_dim：输入嵌入维度（未使用，用 _ 忽略）。
2. 掩码（Mask）处理
python
if mask is not None:
    mask = self.prepare_mask(mask, query.shape, key.shape)
mask 的作用：
屏蔽无效位置（如填充符 PAD 或未来 token）。
形状通常为 (batch_size, key_seq_len, query_seq_len) 或其广播形式（如 (1, key_seq_len, 1)）。
prepare_mask 方法：
确保 mask 的形状与 query 和 key 兼容（如广播或调整维度）。
3. Query、Key、Value 投影
python
query = self.query(query)  # 形状: (seq_len, batch_size, num_heads * d_k)
key = self.key(key)        # 形状: (key_seq_len, batch_size, num_heads * d_k)
value = self.value(value)  # 形状: (key_seq_len, batch_size, num_heads * d_v)
线性变换：
self.query、self.key、self.value 是 nn.Linear 层，将输入投影到多头空间。
投影后形状：(seq_len, batch_size, num_heads * head_dim)。
多头拆分：
通常在后续操作中通过 view 拆分为 (seq_len, batch_size, num_heads, head_dim)（此处未显式写出，可能在 get_score 中处理）。
4. 注意力分数计算
python
scores = self.get_score(query, key)  # 形状: (query_seq_len, key_seq_len, batch_size, num_heads)
get_score 方法：
计算 query 和 key 的点积注意力分数。
通常实现为：
python
# 假设 query 和 key 已拆分为多头
scores = torch.einsum("ibhd,jbhd->ijbh", query, key)  # 形状: (i, j, b, h)
输出形状：(query_seq_len, key_seq_len, batch_size, num_heads)。
5. 缩放注意力分数
python
scores *= self.scale  # scale = 1 / sqrt(d_k)
缩放目的：
防止点积结果过大导致 Softmax 梯度消失。
self.scale 通常设为 1 / sqrt(d_k)（d_k 是 key 的每个注意力头的维度）。
6. 掩码应用
python
if mask is not None:
    scores = scores.masked_fill(mask == 0, float('-inf'))
masked_fill 逻辑：
将 mask 中为 0 的位置替换为 -inf，使得 Softmax 后这些位置的权重为 0。
典型掩码类型：
填充掩码（Padding Mask）：屏蔽 PAD token。
因果掩码（Causal Mask）：屏蔽未来 token（用于解码器）。
7. Softmax 归一化
python
attn = self.softmax(scores)  # 形状: (query_seq_len, key_seq_len, batch_size, num_heads)
Softmax 作用：
沿 key_seq_len 维度（dim=1）归一化，使得每行的注意力权重和为 1。
输出形状与 scores 相同。
8. Dropout 正则化
python
attn = self.dropout(attn)
Dropout 目的：
随机丢弃部分注意力权重，防止过拟合。
9. 注意力权重应用（Value 加权求和）
python
x = torch.einsum('ijbh,jbhd->ibhd', attn, value)  # 形状: (seq_len, batch_size, num_heads, d_v)
爱因斯坦求和规则：
ijbh,jbhd->ibhd：对 j（key_seq_len）维度求和，得到加权后的 value。
输出形状：(seq_len, batch_size, num_heads, d_v)。
10. 多头结果合并 & 输出投影
python
x = x.reshape(seq_len, batch_size, -1)  # 形状: (seq_len, batch_size, num_heads * d_v)
return self.output(x)  # 形状: (seq_len, batch_size, output_dim)
合并多头：
将 num_heads 和 d_v 维度合并，恢复为 (seq_len, batch_size, num_heads * d_v)。
输出投影：
self.output 是 nn.Linear 层，将多头结果映射到最终输出维度。
"""