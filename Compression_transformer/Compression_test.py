"""
---
title: å‹ç¼©Transformerå®éªŒ
summary: è¿™ä¸ªå®éªŒåœ¨tiny Shakespeareæ•°æ®é›†ä¸Šè®­ç»ƒå‹ç¼©Transformeræ¨¡å‹
---

# å‹ç¼©Transformerå®éªŒ

è¿™æ˜¯ä¸€ä¸ªå¸¦æ³¨é‡Šçš„PyTorchå®éªŒï¼Œç”¨äºè®­ç»ƒå‹ç¼©Transformeræ¨¡å‹ã€‚
"""
from typing import List, Tuple, NamedTuple

import torch
import torch.nn as nn

from labml import experiment, tracker, monit, logger
from labml.configs import option
from labml.logger import Text
from labml_helpers.metrics.simple_state import SimpleStateModule
from labml_helpers.module import Module
from labml_helpers.train_valid import BatchIndex, hook_model_outputs
from labml_nn.experiments.nlp_autoregression import NLPAutoRegressionConfigs
from labml_nn.transformers.compressive import CompressiveTransformer, AttentionReconstructionLoss, \
    CompressiveTransformerLayer, Conv1dCompression


class CompressedMemory(NamedTuple):
    """
    ## å‹ç¼©è®°å¿†å®¹å™¨
    ğŸ§  ä½¿ç”¨å‘½åå…ƒç»„å­˜å‚¨ä¸¤ç§è®°å¿†ï¼š
    - mem: ä¸»è®°å¿†ï¼ˆçŸ­æœŸè®°å¿†ï¼‰
    - c_mem: å‹ç¼©è®°å¿†ï¼ˆé•¿æœŸè®°å¿†ï¼‰
    """
    mem: List[torch.Tensor]
    c_mem: List[torch.Tensor]


class AutoregressiveModel(Module):
    """
    ## è‡ªå›å½’æ¨¡å‹
    ğŸ§  æ ¸å¿ƒæ¨¡å‹ç»“æ„ï¼ŒåŒ…å«åµŒå…¥å±‚ã€Transformerå’Œè§£ç å™¨
    """

    def __init__(self, n_vocab: int, d_model: int, transformer: CompressiveTransformer):
        super().__init__()
        # è¯åµŒå…¥å±‚
        self.src_embed = nn.Embedding(n_vocab, d_model)
        # å‹ç¼©Transformeræ¨¡å—
        self.transformer = transformer
        # è¾“å‡ºç”Ÿæˆå±‚
        self.generator = nn.Linear(d_model, n_vocab)
        # æ³¨æ„åŠ›æ©ç ç¼“å­˜
        self.mask_x = None  # åºåˆ—è‡ªèº«çš„æ©ç 
        self.mask_mem = None  # è®°å¿†éƒ¨åˆ†çš„æ©ç 

    def forward(self, x: torch.Tensor, mem: CompressedMemory):
        """
        ğŸ§  å‰å‘ä¼ æ’­æµç¨‹ï¼š
        1. å¤„ç†è®°å¿†è¾“å…¥
        2. ç”Ÿæˆæ³¨æ„åŠ›æ©ç 
        3. è¯åµŒå…¥
        4. é€šè¿‡Transformer
        5. ç”Ÿæˆè¾“å‡º
        """
        # è·å–è®°å¿†å’Œå‹ç¼©è®°å¿†
        if mem is not None:
            mem, c_mem = mem.mem, mem.c_mem
        else:
            mem = []
            c_mem = []

        # è®¡ç®—è®°å¿†æ€»é•¿åº¦ï¼ˆç”¨äºæ©ç ç”Ÿæˆï¼‰
        m_len = len(mem[0]) if mem else 0
        if c_mem:
            m_len += len(c_mem[0])

        # ç”Ÿæˆåºåˆ—çš„å› æœæ©ç ï¼ˆé˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯ï¼‰
        if self.mask_x is None or self.mask_x.shape[0] < len(x):
            from labml_nn.transformers.utils import subsequent_mask
            self.mask_x = subsequent_mask(len(x)).to(x.device)
        # ç”Ÿæˆè®°å¿†éƒ¨åˆ†çš„æ©ç ï¼ˆå…¨1è¡¨ç¤ºå®Œå…¨å¯è§ï¼‰
        if self.mask_mem is None or self.mask_mem.shape[1] < m_len or self.mask_mem.shape[0] < len(x):
            self.mask_mem = self.mask_x.new_ones(len(x), m_len, 1)

        # åˆå¹¶è®°å¿†æ©ç å’Œåºåˆ—æ©ç 
        if m_len:
            mask = torch.cat((self.mask_mem[:len(x), :m_len], self.mask_x[:len(x), :len(x)]), dim=1)
        else:
            mask = self.mask_x[:len(x), :len(x)]

        # è¯åµŒå…¥
        x = self.src_embed(x)
        # é€šè¿‡Transformer
        res, mem = self.transformer(x, mem, c_mem, mask)
        # ç”Ÿæˆä¸‹ä¸€ä¸ªtokençš„logits
        res = self.generator(res)

        return res, mem


class Configs(NLPAutoRegressionConfigs):
    """
    ## å®éªŒé…ç½®
    ğŸ§  åŒ…å«æ¨¡å‹è¶…å‚æ•°å’Œè®­ç»ƒè®¾ç½®
    """

    model: AutoregressiveModel

    # æ¨¡å‹ç»´åº¦
    d_model: int = 128
    # æ³¨æ„åŠ›å¤´æ•°
    heads: int = 4
    # Dropoutæ¦‚ç‡
    dropout: float = 0.0
    # å‰é¦ˆå±‚ä¸­é—´ç»´åº¦
    d_ff: int = 256
    # Transformerå±‚æ•°
    n_layers: int = 6
    # è®°å¿†é•¿åº¦
    mem_len: int = 8
    # è®°å¿†çŠ¶æ€ç®¡ç†æ¨¡å—
    memory = SimpleStateModule()
    # æ³¨æ„åŠ›é‡å»ºæŸå¤±
    attention_reconstruction_loss: AttentionReconstructionLoss
    # å‹ç¼©ç‡ï¼ˆæ¯éš”å¤šå°‘æ­¥å‹ç¼©ä¸€æ¬¡ï¼‰
    compression_rate: int = 4
    # å‹ç¼©è®°å¿†é•¿åº¦
    c_mem_len: int = 128

    def init(self):
        """
        ğŸ§  åˆå§‹åŒ–è·Ÿè¸ªå™¨å’ŒçŠ¶æ€æ¨¡å—
        """
        # é…ç½®è·Ÿè¸ªæŒ‡æ ‡
        tracker.set_scalar("accuracy.*", True)
        tracker.set_scalar("loss.*", True)
        # ä¸åœ¨ç»ˆç«¯æ˜¾ç¤ºæ³¨æ„åŠ›é‡å»ºæŸå¤±
        tracker.set_scalar("ar_loss.*", False)
        # æ·»åŠ é’©å­è®°å½•æ¨¡å‹è¾“å‡º
        hook_model_outputs(self.mode, self.model, 'model')
        # ä¿æŒè®­ç»ƒå’ŒéªŒè¯çš„å‡†ç¡®ç‡å’Œè®°å¿†çŠ¶æ€åˆ†ç¦»
        self.state_modules = [self.accuracy, self.memory]

    @torch.no_grad()
    def merge_compress_memory(self, mem: CompressedMemory, new_mem: List[torch.Tensor]) \
            -> Tuple[CompressedMemory, List[torch.Tensor]]:
        """
        ## åˆå¹¶å’Œå‹ç¼©è®°å¿†
        ğŸ§  æ ¸å¿ƒè®°å¿†ç®¡ç†é€»è¾‘ï¼š
        1. åˆå¹¶æ–°è®°å¿†
        2. æ£€æŸ¥æ˜¯å¦è¶…é™
        3. å‹ç¼©æ—§è®°å¿†
        4. ç»´æŠ¤å‹ç¼©è®°å¿†é˜Ÿåˆ—
        """

        # å¦‚æœé…ç½®ä¸ºä¸ä½¿ç”¨è®°å¿†
        if self.mem_len == 0 and self.c_mem_len == 0:
            return CompressedMemory([], []), []

        # è§£æ„è®°å¿†
        if mem is not None:
            mem, c_mem = mem.mem, mem.c_mem
        else:
            mem, c_mem = [], []

        # åˆå¹¶æ–°è®°å¿†åˆ°ä¸»è®°å¿†
        if mem:
            mem = [torch.cat((m, x), dim=0) for m, x in zip(mem, new_mem)]
        else:
            mem = new_mem

        # å¦‚æœä¸»è®°å¿†è¶…è¿‡é™åˆ¶é•¿åº¦
        if len(mem[0]) > self.mem_len:
            # è®¡ç®—éœ€è¦å‹ç¼©çš„è®°å¿†å—æ•°
            n_c_mem = (len(mem[0]) - self.mem_len + self.compression_rate - 1) // self.compression_rate
            # è®¡ç®—å®é™…è¦å‹ç¼©çš„è®°å¿†é•¿åº¦
            n_old = n_c_mem * self.compression_rate

            # å¾…å‹ç¼©çš„è®°å¿†
            mem_to_compress = []
            # ä¸å‹ç¼©çš„è®°å¿†
            uncompressed_mem = []

            # åˆ†å‰²è®°å¿†
            for m in mem:
                cm, m = torch.split(m, [n_old, len(m) - n_old])
                mem_to_compress.append(cm)
                uncompressed_mem.append(m)
            mem = uncompressed_mem

            # å‹ç¼©è®°å¿†
            new_c_mem = []
            for i, layer in enumerate(self.model.transformer.layers):
                new_c_mem.append(layer.compress(mem_to_compress[i]))

            # åˆå¹¶æ–°æ—§å‹ç¼©è®°å¿†
            if c_mem:
                c_mem = [torch.cat((m, nm), dim=0) for m, nm in zip(c_mem, new_c_mem)]
            else:
                c_mem = new_c_mem

            # å‹ç¼©è®°å¿†é•¿åº¦é™åˆ¶
            if len(c_mem[0]) > self.c_mem_len:
                c_mem = [m[-self.c_mem_len:] for m in c_mem]
        else:
            mem_to_compress = []

        return CompressedMemory(mem, c_mem), mem_to_compress

    def step(self, batch: any, batch_idx: BatchIndex):
        """
        ## è®­ç»ƒ/éªŒè¯æ­¥éª¤
        ğŸ§  å•æ‰¹æ¬¡å¤„ç†æµç¨‹ï¼š
        1. æ•°æ®å‡†å¤‡
        2. è®°å¿†å¤„ç†
        3. æŸå¤±è®¡ç®—
        4. åå‘ä¼ æ’­ï¼ˆè®­ç»ƒæ¨¡å¼ï¼‰
        """

        # æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
        data, target = batch[0].to(self.device), batch[1].to(self.device)

        # è®­ç»ƒæ¨¡å¼ä¸‹æ›´æ–°å…¨å±€æ­¥æ•°
        if self.mode.is_train:
            tracker.add_global_step(data.shape[0] * data.shape[1])

        # æ¨¡å‹å‰å‘ä¼ æ’­
        with self.mode.update(is_log_activations=batch_idx.is_last):
            # è·å–è®°å¿†
            mem = self.memory.get()
            # æ¨¡å‹æ¨ç†
            output, new_mem = self.model(data, mem)
            # åˆå¹¶å‹ç¼©è®°å¿†
            mem, mem_to_compress = self.merge_compress_memory(mem, new_mem)
            # æ›´æ–°è®°å¿†
            self.memory.set(mem)

        # è®¡ç®—äº¤å‰ç†µæŸå¤±
        loss = self.loss_func(output, target)
        tracker.add("loss.", loss)

        # å¦‚æœæœ‰è®°å¿†è¢«å‹ç¼©ï¼Œè®¡ç®—é‡å»ºæŸå¤±
        if mem_to_compress:
            ar_loss = self.attention_reconstruction_loss(new_mem, mem_to_compress)
            tracker.add("ar_loss.", ar_loss)
            loss = loss + ar_loss  # æ€»æŸå¤±

        # è®¡ç®—å‡†ç¡®ç‡
        self.accuracy(output, target)
        self.accuracy.track()

        # è®­ç»ƒæ¨¡å¼ä¸‹çš„åå‘ä¼ æ’­
        if self.mode.is_train:
            loss.backward()
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.grad_norm_clip)
            self.optimizer.step()
            # æ¯ä¸ªepochæœ€åè®°å½•æ¨¡å‹çŠ¶æ€
            if batch_idx.is_last:
                tracker.add('model', self.model)
            self.optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦

        tracker.save()  # ä¿å­˜æŒ‡æ ‡

    def sample(self):
        """
        ## ç”Ÿæˆæ ·æœ¬
        ğŸ§  è®­ç»ƒè¿‡ç¨‹ä¸­å®šæœŸç”Ÿæˆæ–‡æœ¬ç¤ºä¾‹
        """

        # èµ·å§‹æç¤º
        prompt = self.prompt
        # æ”¶é›†è¾“å‡ºç”¨äºæ‰“å°
        log = [(prompt, Text.subtle)]
        # åˆå§‹åŒ–è®°å¿†
        mem = CompressedMemory([], [])
        # ç”Ÿæˆ25ä¸ªtoken
        for i in monit.iterate('Sample', 25):
            # æ–‡æœ¬è½¬token
            data = self.text.text_to_i(prompt).unsqueeze(-1)
            data = data.to(self.device)
            # æ¨¡å‹é¢„æµ‹
            output, new_mem = self.model(data, mem)
            # è´ªå¿ƒè§£ç 
            output = output.argmax(dim=-1).squeeze(1)
            # æ›´æ–°æç¤ºæ–‡æœ¬
            prompt += self.prompt_separator + self.text.itos[output[-1]]
            # ä¸‹æ¬¡è¿­ä»£åªä½¿ç”¨æœ€åä¸€ä¸ªå­—ç¬¦
            prompt = prompt[-1:]
            # è®°å½•è¾“å‡º
            log += [(self.prompt_separator + self.text.itos[output[-1]], Text.value)]
            # æ›´æ–°è®°å¿†
            mem, _ = self.merge_compress_memory(mem, new_mem)

        logger.log(log)  # æ‰“å°ç”Ÿæˆç»“æœ


@option(Configs.model)
def autoregressive_model(c: Configs):
    """
    ## åˆå§‹åŒ–è‡ªå›å½’æ¨¡å‹
    ğŸ§  åˆ›å»ºåŒ…å«ç›¸å¯¹ä½ç½®ç¼–ç çš„å‹ç¼©Transformer
    """
    from labml_nn.transformers.xl import RelativeMultiHeadAttention
    from labml_nn.transformers.feed_forward import FeedForward
    m = AutoregressiveModel(c.n_tokens, c.d_model, CompressiveTransformer(
        CompressiveTransformerLayer(d_model=c.d_model,
                                    self_attn=RelativeMultiHeadAttention(c.heads, c.d_model, c.dropout),
                                    feed_forward=FeedForward(c.d_model, c.d_ff, c.dropout),
                                    dropout_prob=c.dropout,
                                    compress=Conv1dCompression(c.compression_rate, c.d_model)), c.n_layers))
    return m.to(c.device)


@option(Configs.attention_reconstruction_loss)
def attention_reconstruction_loss(c: Configs):
    """
    ## åˆå§‹åŒ–æ³¨æ„åŠ›é‡å»ºæŸå¤±
    ğŸ§  ç¡®ä¿å‹ç¼©åçš„è®°å¿†èƒ½ä¿ç•™åŸå§‹ä¿¡æ¯
    """
    return AttentionReconstructionLoss(c.model.transformer.layers)


def main():
    """
    ## è¿è¡Œå®éªŒ
    ğŸ§  ä¸»å‡½æ•°æµç¨‹ï¼š
    1. åˆ›å»ºå®éªŒ
    2. åŠ è½½é…ç½®
    3. å¯åŠ¨è®­ç»ƒ
    """
    # åˆ›å»ºå®éªŒ
    experiment.create(name="compressive_transformer", comment='')
    # åˆ›å»ºé…ç½®
    conf = Configs()
    # åŠ è½½é…ç½®
    experiment.configs(conf,
                       {'tokenizer': 'character',
                        'text': 'tiny_shakespeare',
                        'optimizer.learning_rate': 2.5e-4,
                        'optimizer.optimizer': 'AdamW',
                        'prompt': 'It is',
                        'prompt_separator': '',

                        'train_loader': 'sequential_train_loader',
                        'valid_loader': 'sequential_valid_loader',

                        'seq_len': 8,
                        'mem_len': 8,
                        'epochs': 128,
                        'batch_size': 32,
                        'inner_iterations': 25,
                        'compression_rate': 2,
                        })

    # è®¾ç½®æ¨¡å‹ä¿å­˜
    experiment.add_pytorch_models({'model': conf.model})

    # å¯åŠ¨å®éªŒ
    with experiment.start():
        conf.run()


if __name__ == '__main__':
    main()