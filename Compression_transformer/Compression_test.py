from typing import List,Tuple,NamedTuple

import  torch
import torch.nn as nn

from labml import monit,experiment,tracker,logger
from labml.configs import option
from labml.logger import Text
from labml_helpers.metrics.simple_state import SimpleStateModule
from labml_helpers.module import Module
from labml_helpers.train_valid import BatchIndex,hook_model_outputs
from labml_nn.experiments.nlp_autoregression import NLPAutoRegressionConfigs
from labml_nn.transformers.compressive import CompressiveTransformer, AttentionReconstructionLoss, \
    CompressiveTransformerLayer, Conv1dCompression
from snappy import uncompress
from statsmodels.sandbox.distributions.examples.matchdist import targetdist


# å°è£…æ¨¡å‹çš„è®°å¿†çŠ¶æ€
class CompressedMemory(NamedTuple):
    mem:List[torch.Tensor]   # ä¸»è®°å¿†ï¼ˆå­˜å‚¨å†å²åºåˆ—çš„å‹ç¼©è¡¨ç¤ºï¼‰
    c_mem:List[torch.TEnsor]  # å‹ç¼©è®°å¿†ï¼ˆè¿›ä¸€æ­¥å‹ç¼©çš„é•¿æœŸè®°å¿†ï¼‰


class AutoregressiveModel(Module):
    def __init__(self,n_vocab:int,d_model:int,transform:CompressiveTransformer):
        super().__init__()
        self.src_embd=nn.Embedding(n_vocab,d_model)  # è¯åµŒå…¥å±‚,æŠŠå•è¯å˜æˆæ•°å­—å¯†ç 
        self.transform=transform   # å‹ç¼©transformæ¨¡å— ï¼Œå¤„ç†ä¿¡æ¯çš„æ ¸å¿ƒå¤§è„‘
        self.generator=nn.Linear(d_model,n_vocab)  # è¾“å‡ºå±‚ï¼Œè¾“å‡ºè¯çš„ç”Ÿæˆæ¦‚ç‡ï¼ŒæŠŠå¤§è„‘ä¸­çš„è¾“å‡ºå˜æˆç­”æ¡ˆ

        self.mask_x=None  # åºåˆ—è‡ªèº«çš„æ³¨æ„åŠ›æ©ç ï¼ˆé˜²æ­¢æœªæ¥ä¿¡æ¯æ³„éœ²ï¼‰ä¸‹ä¸‰è§’çŸ©é˜µ
        self.mask_mem=None  # è®°å¿†éƒ¨åˆ†çš„æ³¨æ„åŠ›æ©ç   å…¨1çŸ©é˜µ

    def forward(self,x:torch.Tensor,mem:CompressedMemory):
        if mem is not None:  # æ£€æŸ¥æ˜¯å¦æœ‰è®°å¿†
            mem,c_mem=mem.mem,mem.c_mem  # æ‰“å¼€è®°å¿†ç›’å­
        else:
            mem=[]  # åˆå§‹åŒ–è®°å¿†ï¼ˆæ¯”å¦‚æ–°å­¦ç”Ÿå¯¹è¿™é‡Œæ˜¯æ²¡æœ‰è®°å¿†çš„ï¼‰
            c_mem=[]

        # mask_xçš„shapeï¼ˆå½“å‰åºåˆ—é•¿åº¦ï¼Œå½“å‰åºåˆ—é•¿åº¦ï¼‰
        # mask_memçš„shapeï¼ˆå½“å‰åºåˆ—é•¿åº¦ï¼Œè®°å¿†é•¿åº¦ï¼‰
        m_len=mem[0].shape[1] if mem else 0  # è®°å½•è®°å¿†é•¿åº¦

        if self.mask_x is None or self.mask_x.shape[0]<len(x): # å¦‚æœå½“å‰æ©ç åºåˆ—ä¸å¤Ÿ
            from labml_nn.transformers.utils import subsequent_mask
            self.mask_x=subsequent_mask(len(x)).to(x.device)  # é‡æ–°ç”Ÿæˆæ©ç ï¼ˆå½“å‰åºåˆ—é•¿åº¦ï¼Œå½“å‰åºåˆ—é•¿åº¦ï¼‰

        if self.mask_mem is None or self.mask_mem.shape[1]<m_len or self.mask_mem.shape[0]<len(x): # å¦‚æœè®°å¿†æ©ç é•¿åº¦ä¸å¤Ÿ
            self.mask_mem=self.mask_x.new_ones(len(x),m_len,1)  # é‡æ–°ç”Ÿæˆå…¨1çŸ©é˜µ

        if m_len:
            # æ‹¼æ¥æ©ç çŸ©é˜µï¼Œä¿è¯èƒ½çœ‹åˆ°è®°å¿†ä¸­çš„å•è¯å’Œå½“å‰çš„å•è¯
            mask=torch.cat((self.mask_mem[:len(x),:m_len],self.mask_x[:len(x),:len(x)]),dim=1)
        else:
            # æ²¡æœ‰è®°å¿†æ—¶é—´ç›´æ¥ä½¿å½“å‰è‡ªèº«çš„æ©ç ï¼ˆå½“å‰åºåˆ—é•¿åº¦ï¼Œå½“å‰åºåˆ—é•¿åº¦ï¼‰
            mask=self.mask_x[:len(x),:len(x)]
        # å°†å•è¯å˜æˆæ•°å­—åºåˆ—
        x=self.src_embd(x)
        # å¼€å§‹æ€è€ƒ
        res,mem=self.transform(x,mem,c_mem,mask)
        # è¾“å‡ºç­”æ¡ˆ
        res=self.generator(res)

        return res,CompressedMemory(mem, c_mem)

# é…ç½®æ¨¡å—
class Configs(NLPAutoRegressionConfigs):
    model:AutoregressiveModel # è‡ªå›å½’æ¨¡å‹
    d_model:int=128  # éšè—å±‚æ•°é‡
    heads:int=4  # Transformerä¸­çš„å¤šå¤´æ•°
    dropout:float=0.0  # dropoutæ¦‚ç‡
    d_ff:int=256  #  FeedForwardä¸­çš„ä¸­é—´å±‚æ•°é‡
    n_layers:int=6  # Transformerå±‚æ•°
    mem_len:int=8  # ä¸»è®°å¿†æœ€å¤§é•¿åº¦
    memory=SimpleStateModule()  # è®°å¿†çŠ¶æ€ç®¡ç†æ¨¡å—
    attention_reconstruction_loss:AttentionReconstructionLoss  # æ³¨æ„åŠ›é‡å»ºæŸå¤±
    compression_rate:int=4  # è®°å¿†å‹ç¼©æ¦‚ç‡ï¼ˆæ¯éš”å¤šå°‘æ­¥å‹ç¼©ä¸€æ¬¡ï¼‰
    c_mem_len:int=128  # å‹ç¼©è®°å¿†çš„æœ€å¤§é•¿åº¦

    def init(self):
        tracker.set_scalar('ar_loss.*',True)  # è·Ÿè¸ªå›å½’æŸå¤±
        tracker.set_scalar('loss.*',True) # è·Ÿè¸ªæ€»æŸå¤±
        tracker.set_scalar('ar_loss.*',False) # ä¸æ‰“å°æ—¥å¿—è¾“å‡º

        hook_model_outputs(self.model,self.model,'model')  # é’©å­å‡½æ•°ç›‘æ§æ¨¡å‹è¾“å‡º
        self.state_modules=[self.accuracy,self.memory]  # çŠ¶æ€ç®¡ç†æ¨¡å—
    # åˆå¹¶æ–°è®°å¿†å’Œè€è®°å¿†
    @torch.no_grad()
    def merge_compress_memory(self,
                              mem:CompressedMemory,new_mem:List[torch.Tensor])->Tuple[CompressedMemory,List[torch.Tensor]]:
        if self.mem_len==0 and self.c_mem_len==0:
            # å¦‚æœmem_lenå’Œc_mem_lenéƒ½æ˜¯0è¡¨ç¤ºä¸å¯ç”¨è®°å¿†ï¼Œè¿”å›ç©º
            return CompressedMemory([],[]),[]

        # åˆå§‹åŒ–è®°å¿†
        if mem:
            mem,c_mem=mem.mem,mem.c_mem
        else:
            mem=[]
            c_mem=[]

        # åˆå¹¶æ–°è®°å¿†
        if mem:
            # å¦‚æœæœ‰ä»¥å‰çš„è®°å¿†ï¼Œè®²æ–°çš„è®°å¿†æ‹¼æ¥åœ¨è€è®°å¿†çš„åé¢
            mem=[torch.cat((m,x),dim=0) for m,x in zip(mem,new_mem)]
        else:
            mem=new_mem

        # å¦‚æœè®°å¿†çš„é•¿åº¦å¤§äºäº†ä¸»è®°å¿†çš„æœ€å¤§é•¿åº¦
        if len(mem[0])>self.mem_len:
            # è®¡ç®—éœ€è¦å‹ç¼©çš„è®°å¿†å—çš„æ•°é‡
            n_c_mem=(len(mem[0])-self.mem_len+self.compression_rate-1)//self.compression_rate
            # è®¡ç®—éœ€è¦å‹ç¼©çš„è®°å¿†é•¿åº¦ï¼ˆcompression=4ï¼Œè¡¨ç¤ºæ²¡å››æ­¥å‹ç¼©ä¸€æ¬¡ï¼‰
            n_old=n_c_mem*self.compression_rate

            # åˆ†å‰²è®°å¿†
            mem_to_compress=[]  #å¸¦å‹ç¼©çš„è®°å¿†
            uncompress_mem=[] # ä¸å‹ç¼©çš„è®°å¿†

            for m in mem:
                cm,m=torch.split(m,[n_old,len(m)-n_old])
                mem_to_compress.append(cm)
                uncompress_mem.append(m)
            mem=uncompress_mem # æ›´æ–°ä¸»è®°å¿†

            # å‹ç¼©è®°å¿†
            new_c_mem=[]

            for i,layer in enumerate(self.model.transform.layers):
                new_c_mem.append(layer.compress(mem_to_compress[i]))  # è°ƒç”¨å‹ç¼©å‡½æ•°

            if c_mem:
                c_mem=[torch.cat((m,nm),dim=0) for m ,nm in zip(c_mem,new_c_mem)]
            else:
                c_mem=new_c_mem

            if len(c_mem[0])>self.c_mem_len:
                c_mem=[m[-self.c_mem_len:] for m in c_mem]
        else:
            mem_to_compress=[]

        return CompressedMemory(mem, c_mem),mem_to_compress

    def step(self, batch: any, batch_idx: BatchIndex):
        """
        è´Ÿè´£å•æ‰¹æ¬¡è®­ç»ƒ/éªŒè¯çš„æ ¸å¿ƒé€»è¾‘
        :param batch: ä½†é’±æ‰¹æ¬¡æ•°æ®ï¼ˆè¾“å…¥åºåˆ—ï¼Œç›®æ ‡åºåˆ—ï¼‰
        :param batch_idx: æ‰¹æ¬¡ç´¢å¼•å¯¹è±¡ï¼ˆis_trainï¼šæ˜¯å¦æ˜¯è®­ç»ƒæ¨¡å¼ï¼Œis_lastï¼šæ˜¯å¦æ˜¯ä½†é’±epochçš„æœ€åä¸€æ‰¹ï¼‰
        :return:
        """
        # è®²æ•°æ®ç§»åŠ¨åˆ°GPUæˆ–è€…CPUä¸­
        data,target=batch[0].to(self.device),batch[1].to(self.device)
        # è®­ç»ƒæ¨¡å¼ä¸‹çš„å…¨å±€æ­¥æ•°æ›´æ–°
        if self.mode.is_train:
            # ç»Ÿè®¡å·²å¤„ç†çš„tokenæ€»æ•°ï¼Œbatch_size*seq_len
            tracker.add_global_step(data.shape[0]*data.shape[1])
        # æ¨¡å‹å‰å‘ä¼ æ’­
        with  self.mode.update(is_log_activations=batch_idx.is_last):
            # è·å–å½“å‰è®°å¿†
            mem=self.memory.get()
            # è®²è®°å¿†ä¼ å…¥æ¨¡å‹ä¸­æ¨ç†è¿”å›æ–°çš„è®°å¿†
            output,new_men=self.model(data,mem)
            # åˆå¹¶å‹ç¼©è®°å¿†
            mem,mem_to_compress=self.merge_compress_memory(mem,new_men)
            # æ›´æ–°è®°å¿†çŠ¶æ€
            self.memory.set(mem)
        # äº¤å‰ç†µè®¡ç®—æŸå¤±
        loss=self.loss_func(output,target)
        tracker.add('loss.',loss)
        # å¦‚æœè®°å¿†è¢«å‹ç¼©
        if mem_to_compress:
            ar_loss=self.attention_reconstruction_loss(new_men,mem_to_compress)

            tracker.add('ar_loss.',ar_loss)

            loss+=ar_loss  # æ€»æŸå¤±=ä¸»æŸå¤±+è®°å¿†é‡å»ºæŸå¤±

        self.accuracy(output, target)  # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„å‡†ç¡®ç‡
        self.accuracy.track() # è®°å½•åˆ°track
        # è¿”ç°çš„ç»™ä¼ æ’­
        if self.mode.is_train:
            loss.backward()    # åå‘ä¼ æ’­
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.grad_norm_clip)
            self.optimizer.step() # å‚æ•°æ›´æ–°
            if batch_idx.is_last:  # å¦‚æœæ˜¯æœ€åä¸€æ‰¹
                tracker.add('model',self.model)  # è®°å½•æ¨¡å‹çŠ¶æ€

            # æ¢¯åº¦æ¸…é›¶
            self.optimizer.zero_grad()
        # ä¿å­˜ç»“æœå°†è·Ÿè¸ªæ•°æ®å†™å…¥æ—¥å¿—
        tracker.save()

    def sample(self):
        """
        ## ç”Ÿæˆæ ·æœ¬
        ğŸ§  è®­ç»ƒè¿‡ç¨‹ä¸­å®šæœŸç”Ÿæˆæ–‡æœ¬ç¤ºä¾‹
        """

        # èµ·å§‹æç¤º
        prompt = self.prompt
        # æ”¶é›†è¾“å‡ºç”¨äºæ‰“å°
        log = [(prompt, Text.subtle)]
        # åˆå§‹åŒ–è®°å¿†
        mem = CompressedMemory([], [])
        # ç”Ÿæˆ25ä¸ªtoken
        for i in monit.iterate('Sample', 25):
            # æ–‡æœ¬è½¬token
            data = self.text.text_to_i(prompt).unsqueeze(-1)
            data = data.to(self.device)
            # æ¨¡å‹é¢„æµ‹
            output, new_mem = self.model(data, mem)
            # è´ªå¿ƒè§£ç 
            output = output.argmax(dim=-1).squeeze(1)
            # æ›´æ–°æç¤ºæ–‡æœ¬
            prompt += self.prompt_separator + self.text.itos[output[-1]]
            # ä¸‹æ¬¡è¿­ä»£åªä½¿ç”¨æœ€åä¸€ä¸ªå­—ç¬¦
            prompt = prompt[-1:]
            # è®°å½•è¾“å‡º
            log += [(self.prompt_separator + self.text.itos[output[-1]], Text.value)]
            # æ›´æ–°è®°å¿†
            mem, _ = self.merge_compress_memory(mem, new_mem)

        logger.log(log)  # æ‰“å°ç”Ÿæˆç»“æœ

@option(Configs.model)
def autoregressive_model(c: Configs):
    """
    ## åˆå§‹åŒ–è‡ªå›å½’æ¨¡å‹
    ğŸ§  åˆ›å»ºåŒ…å«ç›¸å¯¹ä½ç½®ç¼–ç çš„å‹ç¼©Transformer
    """
    from labml_nn.transformers.xl import RelativeMultiHeadAttention
    from labml_nn.transformers.feed_forward import FeedForward
    m = AutoregressiveModel(c.n_tokens, c.d_model, CompressiveTransformer(
        CompressiveTransformerLayer(d_model=c.d_model,
                                    self_attn=RelativeMultiHeadAttention(c.heads, c.d_model, c.dropout),
                                    feed_forward=FeedForward(c.d_model, c.d_ff, c.dropout),
                                    dropout_prob=c.dropout,
                                    compress=Conv1dCompression(c.compression_rate, c.d_model)), c.n_layers))
    return m.to(c.device)

@option(Configs.attention_reconstruction_loss)
def attention_reconstruction_loss(c: Configs):
    """
    ## åˆå§‹åŒ–æ³¨æ„åŠ›é‡å»ºæŸå¤±
    ğŸ§  ç¡®ä¿å‹ç¼©åçš„è®°å¿†èƒ½ä¿ç•™åŸå§‹ä¿¡æ¯
    """
    return AttentionReconstructionLoss(c.model.transformer.layers)

def main():
    """
    ## è¿è¡Œå®éªŒ
    ğŸ§  ä¸»å‡½æ•°æµç¨‹ï¼š
    1. åˆ›å»ºå®éªŒ
    2. åŠ è½½é…ç½®
    3. å¯åŠ¨è®­ç»ƒ
    """
    # åˆ›å»ºå®éªŒ
    experiment.create(name="compressive_transformer", comment='')
    # åˆ›å»ºé…ç½®
    conf = Configs()
    # åŠ è½½é…ç½®
    experiment.configs(conf,
                       {'tokenizer': 'character',
                        'text': 'tiny_shakespeare',
                        'optimizer.learning_rate': 2.5e-4,
                        'optimizer.optimizer': 'AdamW',
                        'prompt': 'It is',
                        'prompt_separator': '',

                        'train_loader': 'sequential_train_loader',
                        'valid_loader': 'sequential_valid_loader',

                        'seq_len': 8,
                        'mem_len': 8,
                        'epochs': 128,
                        'batch_size': 32,
                        'inner_iterations': 25,
                        'compression_rate': 2,
                        })

    # è®¾ç½®æ¨¡å‹ä¿å­˜
    experiment.add_pytorch_models({'model': conf.model})

    # å¯åŠ¨å®éªŒ
    with experiment.start():
        conf.run()

if __name__ == '__main__':
    main()






















